# -*- coding: utf-8 -*-
"""train lstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lY864bCUPALgxvZSKo7MDgmAOPWPnNiF
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchtext.data as data
import torchtext
import time
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from scipy import sparse
import pandas as pd
import seaborn as sn
import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
import xgboost as xgb
import sys
# from cStringIO import StringIO

BATCH_SIZE = 512

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class SNLI():
  def __init__(self,batch_size= [BATCH_SIZE]*3):
    self.inputs = data.Field(lower = True, tokenize='spacy')
    self.answers = data.Field(sequential = False)
    self.train, self.dev, self.test = torchtext.datasets.SNLI.splits(self.inputs, self.answers)
    self.inputs.build_vocab(self.train,self.dev,vectors = "glove.6B.300d")
    self.answers.build_vocab(self.train)

    self.train_iter, self.dev_iter, self.test_iter = data.Iterator.splits((self.train, self.dev, self.test), batch_sizes=batch_size,device=device)
    # self.train_iter, self.dev_iter, self.test_iter = torchtext.datasets.SNLI.iters(batch_size=128)
  def vocab_size(self):
    return len(self.inputs.vocab)

  def out_dim(self):
    return len(self.answers.vocab)

# class SNLI_RNN(nn.Module):
#   def __init__(self,vocab_size,out_dim,batch_size=BATCH_SIZE,embed_dim=300,proj_dim=300,lstm_hidden_size=512,lstm_layers=3):
#     super().__init__()
        
#     self.batch_size = batch_size
#     self.embedding = nn.Embedding(vocab_size,embed_dim)
#     self.projection = nn.Linear(embed_dim,proj_dim)
#     self.lstm = nn.LSTM(proj_dim, lstm_hidden_size,num_layers=lstm_layers)
#     self.relu = nn.ReLU()
#     self.out = nn.Sequential(
#         nn.Linear(1024,1024),
#         self.relu,
#         nn.Linear(1024,1024),
#         self.relu,
#         nn.Linear(1024,1024),
#         self.relu,
#         nn.Linear(1024,out_dim)
#     )

#   def forward(self,batch):
#       batch.premise = torch.transpose(batch.premise,0,1)
#       batch.hypothesis = torch.transpose(batch.hypothesis,0,1)

#       # print('batch.premise.shape',batch.premise.shape,'batch.hypothesis.shape',batch.hypothesis.shape)
#       # print('batch.label.shape',batch.label.shape)
#       prem_embed = self.embedding(batch.premise)
#       # print('prem_embed.shape',prem_embed.shape)
#       hyp_embed = self.embedding(batch.hypothesis)
#       # print('hyp_embed.shape',hyp_embed.shape)
#       prem_proj = self.relu(self.projection(prem_embed))
#       # print('prem_proj.shape',prem_proj.shape)
#       hyp_proj = self.relu(self.projection(hyp_embed))
#       # print('hyp_proj.shape',hyp_proj.shape)
#       encoded_prem,_ = self.lstm(prem_proj)
#       # print('encoded_prem.shape',encoded_prem.shape)
#       encoded_hyp,_ = self.lstm(hyp_proj)
#       # print('encoded_hyp.shape',encoded_hyp.shape)
#       premise = encoded_prem.sum(dim=1)
#       # print('premise.shape',premise.shape)
#       hypothesis = encoded_hyp.sum(dim=1)
#       # print('hypothesis.shape',hypothesis.shape)
#       combined = torch.cat( (premise,hypothesis),1)
#       # print('combined.shape',combined.shape)
#       # premise = premise.view(self.batch_size,-1)
#       # print('premise.shape',premise.shape)
#       # hypothesis = hypothesis.view(self.batch_size,-1)
#       # print('hypothesis.shape',hypothesis.shape)
#       # combined = torch.cat( (premise,hypothesis))
#       # print('combined.shape',combined.shape)
#       # combined = combined.view(-1,128)
#       return self.out(combined)

snli = SNLI()

# snli_rnn = SNLI_RNN(snli.vocab_size(),snli.out_dim())
# # # print(snli_rnn)

class NLIBiLSTM(nn.Module):
    def __init__(self, 
                 input_dim,                  
                 output_dim
                 ):
        
        super().__init__()

        embedding_dim=300
        hidden_dim=300
        n_lstm_layers=3
        n_fc_layers=2                               
        dropout=0.25

        self.embedding = nn.Embedding(input_dim, embedding_dim)
        #, padding_idx = pad_idx)
        
        self.translation = nn.Linear(embedding_dim, hidden_dim)
        
        self.lstm = nn.LSTM(hidden_dim, 
                            hidden_dim, 
                            num_layers = n_lstm_layers, 
                            bidirectional = True, 
                            dropout=dropout if n_lstm_layers > 1 else 0)
        
        fc_dim = hidden_dim * 2
        
        fcs = [nn.Linear(fc_dim * 2, fc_dim * 2) for _ in range(n_fc_layers)]
        
        self.fcs = nn.ModuleList(fcs)
        
        self.fc_out = nn.Linear(fc_dim * 2, output_dim)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, prem, hypo):

        prem_seq_len, batch_size = prem.shape
        hypo_seq_len, _ = hypo.shape
        
        #prem = [prem sent len, batch size]
        #hypo = [hypo sent len, batch size]
        
        embedded_prem = self.embedding(prem)
        embedded_hypo = self.embedding(hypo)
        
        #embedded_prem = [prem sent len, batch size, embedding dim]
        #embedded_hypo = [hypo sent len, batch size, embedding dim]
        
        translated_prem = F.relu(self.translation(embedded_prem))
        translated_hypo = F.relu(self.translation(embedded_hypo))
        
        #translated_prem = [prem sent len, batch size, hidden dim]
        #translated_hypo = [hypo sent len, batch size, hidden dim]
        
        outputs_prem, (hidden_prem, cell_prem) = self.lstm(translated_prem)
        outputs_hypo, (hidden_hypo, cell_hypo) = self.lstm(translated_hypo)

        #outputs_x = [sent len, batch size, n directions * hid dim]
        #hidden_x = [n layers * n directions, batch size, hid dim]
        #cell_x = [n layers * n directions, batch size, hid dim]
        
        hidden_prem = torch.cat((hidden_prem[-1], hidden_prem[-2]), dim=-1)
        hidden_hypo = torch.cat((hidden_hypo[-1], hidden_hypo[-2]), dim=-1)
        
        #hidden_x = [batch size, fc dim]

        hidden = torch.cat((hidden_prem, hidden_hypo), dim=1)

        #hidden = [batch size, fc dim * 2]
            
        for fc in self.fcs:
            hidden = fc(hidden)
            hidden = F.relu(hidden)
            hidden = self.dropout(hidden)
        
        prediction = self.fc_out(hidden)
        
        #prediction = [batch size, output dim]
        
        return prediction

INPUT_DIM = snli.vocab_size()
OUTPUT_DIM = snli.out_dim()

snli_rnn = NLIBiLSTM(input_dim = INPUT_DIM,
                  output_dim = OUTPUT_DIM
                  ).to(device)

EPOCHS = 10
LEARNING_RATE = 0.001
optimizer = optim.Adam(snli_rnn.parameters(),lr=LEARNING_RATE)
loss_fn = nn.CrossEntropyLoss()

def train(network,loader):
  # total_loss_list = []
  mean_loss_list = []

  for epoch in range(EPOCHS):
    loader.init_epoch()
    epoch_start_time = time.time()
    total_loss = 0
    for batch_idx,batch in enumerate(loader):
      start = time.time()
      prediction = network(batch)
      loss = loss_fn(prediction,batch.label)

      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

      total_loss += loss.item()

      if epoch %2 == 0:
        print('Epoch#',epoch+1,'Batch#',batch_idx+1,'Loss',round(total_loss,3),'Time',time.time()-start)

    if epoch %2 == 0:
        print('Epoch#',epoch+1,'Loss',round(total_loss,3),'Time',time.time()-epoch_start_time)

  mean_loss = total_loss/len(batch)
  mean_loss_list.append(mean_loss)  
  return mean_loss_list

def calc_accuracy(network,loader):
  total,correct = 0,0
  snli_rnn.eval()
  loader.init_epoch()
  for batch_idx,batch in enumerate(loader):
    prediction = network(batch)
    _,predicted = torch.max(prediction.data,1)
    correct += (predicted == batch.label).sum()
    total += len(batch.label)

  accuracy = 100. * correct/total
  return accuracy

train(snli_rnn, snli.train_iter)

torch.save(snli_rnn,"model/bilstm.pth")

snli_rnn = torch.load("model/bilstm.pth")

# import pandas as pd

# test_df = pd.read_csv(test_path)
#Output labels
index2label = ['neutral', 'entailment','contradiction','-']
f = open('deep_model.txt','w')
for batch in snli.test_iter:
# for index,batch in test_df.iterrows():
  predicted = snli_rnn(batch['sentence1'],batch['sentence2'])

  for label in predicted:
    print(label)
    _,label = label.max(0)
    print(label)
    f.write(index2label[label]+"\n")