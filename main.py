# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mnckPP147APA2VnTb8-3LlJZniP26_dR
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchtext.data as data
import torchtext
import time
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from scipy import sparse
import pandas as pd
import seaborn as sn
import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
import xgboost as xgb
import sys
# from cStringIO import StringIO

# from google.colab import drive
# drive.mount('/content/gdrive',force_remount=True)
train_csv_file = "snli_1.0_train.csv"
test_csv_file = 'snli_1.0_test.csv' 
dev_csv_file = 'snli_1.0_dev.csv'

# PATH = "/content/gdrive/My Drive/DL assignment 3/"
# train_path = PATH + train_csv_file 
test_path = test_csv_file
# dev_path = PATH + dev_csv_file

lr = LogisticRegression(multi_class='multinomial')

test_df = pd.read_csv(test_path)

def print_labels(df,lr,vect_word):
  with open('tfidf.txt','w') as f:
    X = get_X(df,vect_word)
    predicted = lr.predict(X)
    index2label = ['neutral', 'entailment','contradiction']
    for label in predicted:
      f.write(index2label[label]+"\n")

import pickle
lr = pickle.load(open("model/logistic_model.sav","rb"))
vect_word = pickle.load(open("model/logistic_vect_word.sav","rb"))
print_labels(test_df,lr,vect_word)

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchtext.data as data
import torchtext
import time
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from scipy import sparse
import pandas as pd
import seaborn as sn
import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
import xgboost as xgb
import sys
# from cStringIO import StringIO

BATCH_SIZE = 512

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class SNLI():
  def __init__(self,batch_size= [BATCH_SIZE]*3):
    self.inputs = data.Field(lower = True, tokenize='spacy')
    self.answers = data.Field(sequential = False)
    self.train, self.dev, self.test = torchtext.datasets.SNLI.splits(self.inputs, self.answers)
    self.inputs.build_vocab(self.train,self.dev,vectors = "glove.6B.300d")
    self.answers.build_vocab(self.train)

    self.train_iter, self.dev_iter, self.test_iter = data.Iterator.splits((self.train, self.dev, self.test), batch_sizes=batch_size,device=device)
    # self.train_iter, self.dev_iter, self.test_iter = torchtext.datasets.SNLI.iters(batch_size=128)
  def vocab_size(self):
    return len(self.inputs.vocab)

  def out_dim(self):
    return len(self.answers.vocab)

snli = SNLI()

class NLIBiLSTM(nn.Module):
    def __init__(self, 
                 input_dim,                  
                 output_dim
                 ):
        
        super().__init__()

        embedding_dim=300
        hidden_dim=300
        n_lstm_layers=3
        n_fc_layers=2                               
        dropout=0.25

        self.embedding = nn.Embedding(input_dim, embedding_dim)
        #, padding_idx = pad_idx)
        
        self.translation = nn.Linear(embedding_dim, hidden_dim)
        
        self.lstm = nn.LSTM(hidden_dim, 
                            hidden_dim, 
                            num_layers = n_lstm_layers, 
                            bidirectional = True, 
                            dropout=dropout if n_lstm_layers > 1 else 0)
        
        fc_dim = hidden_dim * 2
        
        fcs = [nn.Linear(fc_dim * 2, fc_dim * 2) for _ in range(n_fc_layers)]
        
        self.fcs = nn.ModuleList(fcs)
        
        self.fc_out = nn.Linear(fc_dim * 2, output_dim)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, prem, hypo):

        prem_seq_len, batch_size = prem.shape
        hypo_seq_len, _ = hypo.shape
        
        #prem = [prem sent len, batch size]
        #hypo = [hypo sent len, batch size]
        
        embedded_prem = self.embedding(prem)
        embedded_hypo = self.embedding(hypo)
        
        #embedded_prem = [prem sent len, batch size, embedding dim]
        #embedded_hypo = [hypo sent len, batch size, embedding dim]
        
        translated_prem = F.relu(self.translation(embedded_prem))
        translated_hypo = F.relu(self.translation(embedded_hypo))
        
        #translated_prem = [prem sent len, batch size, hidden dim]
        #translated_hypo = [hypo sent len, batch size, hidden dim]
        
        outputs_prem, (hidden_prem, cell_prem) = self.lstm(translated_prem)
        outputs_hypo, (hidden_hypo, cell_hypo) = self.lstm(translated_hypo)

        #outputs_x = [sent len, batch size, n directions * hid dim]
        #hidden_x = [n layers * n directions, batch size, hid dim]
        #cell_x = [n layers * n directions, batch size, hid dim]
        
        hidden_prem = torch.cat((hidden_prem[-1], hidden_prem[-2]), dim=-1)
        hidden_hypo = torch.cat((hidden_hypo[-1], hidden_hypo[-2]), dim=-1)
        
        #hidden_x = [batch size, fc dim]

        hidden = torch.cat((hidden_prem, hidden_hypo), dim=1)

        #hidden = [batch size, fc dim * 2]
            
        for fc in self.fcs:
            hidden = fc(hidden)
            hidden = F.relu(hidden)
            hidden = self.dropout(hidden)
        
        prediction = self.fc_out(hidden)
        
        #prediction = [batch size, output dim]
        
        return prediction

INPUT_DIM = snli.vocab_size()
OUTPUT_DIM = snli.out_dim()

snli_rnn = NLIBiLSTM(input_dim = INPUT_DIM,
                  output_dim = OUTPUT_DIM
                  ).to(device)

snli_rnn = torch.load("model/bilstm.pth")

#Output labels
index2label = ['neutral', 'entailment','contradiction','-']
f = open('deep_model.txt','w')
for batch in snli.test_iter:
  predicted = snli_rnn(batch['sentence1'],batch['sentence2'])

  for label in predicted:
    print(label)
    _,label = label.max(0)
    print(label)
    f.write(index2label[label]+"\n")